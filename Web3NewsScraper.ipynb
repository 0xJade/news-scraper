{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd48437-248a-4e41-b084-db9cf19af9f7",
   "metadata": {},
   "source": [
    "# Ecosystem Report\n",
    "\n",
    "This report aims to scrape blogs and summarize news, generate proposals for how we can present research ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a7cd729-4d20-4019-8b31-abe6052471a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:47:48,851 - INFO - Attempting to find RSS feed for Flow blog...\n",
      "2025-08-31 15:47:49,285 - WARNING - No RSS feed found for Flow blog\n",
      "2025-08-31 15:47:49,287 - INFO - \n",
      "==================================================\n",
      "2025-08-31 15:47:49,288 - INFO - Processing source: ethereum_blog\n",
      "2025-08-31 15:47:49,288 - INFO - URL: https://blog.ethereum.org/feed.xml\n",
      "2025-08-31 15:47:49,289 - INFO - Testing URL accessibility for ethereum_blog: https://blog.ethereum.org/feed.xml\n",
      "2025-08-31 15:47:50,671 - INFO - ethereum_blog - Status Code: 200\n",
      "2025-08-31 15:47:50,673 - INFO - ethereum_blog - Content-Type: application/xml\n",
      "2025-08-31 15:47:50,674 - INFO - ethereum_blog - Content Length: 480357 bytes\n",
      "2025-08-31 15:47:50,674 - INFO - Parsing feed for ethereum_blog...\n",
      "2025-08-31 15:47:51,160 - INFO - Analyzing feed structure for ethereum_blog\n",
      "2025-08-31 15:47:51,161 - INFO - Processing 583 entries from ethereum_blog\n",
      "2025-08-31 15:47:51,161 - INFO - Added upgrade-related article from ethereum_blog: Protocol Update 003 — Improve UX\n",
      "2025-08-31 15:47:51,161 - INFO - Added upgrade-related article from ethereum_blog: Protocol Update 002 - Scale Blobs\n",
      "2025-08-31 15:47:51,161 - INFO - \n",
      "==================================================\n",
      "2025-08-31 15:47:51,162 - INFO - Processing source: arbitrum_medium\n",
      "2025-08-31 15:47:51,162 - INFO - URL: https://medium.com/feed/@arbitrum\n",
      "2025-08-31 15:47:51,162 - INFO - Testing URL accessibility for arbitrum_medium: https://medium.com/feed/@arbitrum\n",
      "2025-08-31 15:47:51,402 - INFO - arbitrum_medium - Status Code: 200\n",
      "2025-08-31 15:47:51,403 - INFO - arbitrum_medium - Content-Type: text/xml; charset=UTF-8\n",
      "2025-08-31 15:47:51,404 - INFO - arbitrum_medium - Content Length: 17843 bytes\n",
      "2025-08-31 15:47:51,408 - INFO - Parsing feed for arbitrum_medium...\n",
      "2025-08-31 15:47:51,664 - INFO - Analyzing feed structure for arbitrum_medium\n",
      "2025-08-31 15:47:51,665 - INFO - Processing 7 entries from arbitrum_medium\n",
      "2025-08-31 15:47:51,665 - INFO - Added upgrade-related article from arbitrum_medium: Most profitable SushiSwap liquidity pool ArbiFLUX-ETH — 162.44% APY\n",
      "2025-08-31 15:47:51,666 - INFO - Added upgrade-related article from arbitrum_medium: Binance, Arbitrum One Integration, Datamine Network\n",
      "2025-08-31 15:47:51,666 - INFO - \n",
      "==================================================\n",
      "2025-08-31 15:47:51,666 - INFO - Total articles found: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Source: ethereum_blog\n",
      "Title: Protocol Update 003 — Improve UX\n",
      "Date: Fri, 29 Aug 2025 00:00:00 GMT\n",
      "Link: https://blog.ethereum.org/en/2025/08/29/protocol-update-003\n",
      "----------------------------------------\n",
      "\n",
      "Source: ethereum_blog\n",
      "Title: Protocol Update 002 - Scale Blobs\n",
      "Date: Fri, 22 Aug 2025 00:00:00 GMT\n",
      "Link: https://blog.ethereum.org/en/2025/08/22/protocol-update-002\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: Most profitable SushiSwap liquidity pool ArbiFLUX-ETH — 162.44% APY\n",
      "Date: Wed, 15 Dec 2021 01:20:00 GMT\n",
      "Link: https://arbitrum.medium.com/most-profitable-sushiswap-liquidity-pool-arbiflux-eth-162-44-apy-8b717e5e7b2d?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: Binance, Arbitrum One Integration, Datamine Network\n",
      "Date: Sat, 20 Nov 2021 00:56:29 GMT\n",
      "Link: https://arbitrum.medium.com/binance-arbitrum-one-integration-datamine-network-a2998644367c?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BlockchainNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.sources = {\n",
    "            'ethereum_blog': 'https://blog.ethereum.org/feed.xml',\n",
    "            'arbitrum_medium': 'https://medium.com/feed/@arbitrum',\n",
    "            # 'polygon_blog': 'https://blog.polygon.technology/feed',\n",
    "            # 'solana_news': 'https://solana.com/news/rss.xml',\n",
    "            # 'flow_blog': 'https://www.onflow.org/post/rss.xml'\n",
    "        }\n",
    "        \n",
    "        # Add headers to mimic a real browser\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'application/rss+xml, application/xml, text/xml, application/atom+xml',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "        }\n",
    "    \n",
    "    def test_url_accessibility(self, url, source_name):\n",
    "        \"\"\"Test if URL is accessible and what type of content it returns\"\"\"\n",
    "        logger.info(f\"Testing URL accessibility for {source_name}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            \n",
    "            logger.info(f\"{source_name} - Status Code: {response.status_code}\")\n",
    "            logger.info(f\"{source_name} - Content-Type: {response.headers.get('content-type', 'Unknown')}\")\n",
    "            logger.info(f\"{source_name} - Content Length: {len(response.content)} bytes\")\n",
    "            \n",
    "            # Check if it's actually RSS/XML content\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            is_xml = 'xml' in content_type or 'rss' in content_type or 'atom' in content_type\n",
    "            \n",
    "            if not is_xml:\n",
    "                # Check if content starts with XML declaration or RSS tags\n",
    "                content_preview = response.text[:200]\n",
    "                logger.info(f\"{source_name} - Content preview: {content_preview}\")\n",
    "                \n",
    "                if not any(tag in content_preview.lower() for tag in ['<rss', '<feed', '<?xml']):\n",
    "                    logger.warning(f\"{source_name} - This appears to be HTML, not RSS/XML!\")\n",
    "            \n",
    "            return response.status_code == 200, response\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"{source_name} - Request failed: {e}\")\n",
    "            return False, None\n",
    "    \n",
    "    def analyze_feed_structure(self, feed, source_name):\n",
    "        \"\"\"Analyze the feed structure to understand what's available\"\"\"\n",
    "        logger.info(f\"Analyzing feed structure for {source_name}\")\n",
    "\n",
    "        if feed.entries:\n",
    "            first_entry = feed.entries[0]\n",
    "        \n",
    "        if hasattr(feed, 'bozo') and feed.bozo:\n",
    "            logger.warning(f\"{source_name} - Feed has parsing issues: {feed.bozo_exception}\")\n",
    "    \n",
    "    def scrape_rss_feeds(self):\n",
    "        articles = []\n",
    "        \n",
    "        for source, url in self.sources.items():\n",
    "            logger.info(f\"\\n{'='*50}\")\n",
    "            logger.info(f\"Processing source: {source}\")\n",
    "            logger.info(f\"URL: {url}\")\n",
    "            \n",
    "            try:\n",
    "                # First, test URL accessibility\n",
    "                is_accessible, response = self.test_url_accessibility(url, source)\n",
    "                \n",
    "                if not is_accessible:\n",
    "                    logger.error(f\"Skipping {source} - URL not accessible\")\n",
    "                    continue\n",
    "                \n",
    "                # Parse the feed\n",
    "                logger.info(f\"Parsing feed for {source}...\")\n",
    "                feed = feedparser.parse(url)\n",
    "                \n",
    "                # Analyze feed structure\n",
    "                self.analyze_feed_structure(feed, source)\n",
    "                \n",
    "                # Check if feed has entries\n",
    "                if len(feed.entries) > 0:\n",
    "                    logger.info(f\"Processing {len(feed.entries)} entries from {source}\")\n",
    "                    \n",
    "                    for i, entry in enumerate(feed.entries[:3]):  # Latest 3 articles\n",
    "                        logger.debug(f\"Processing entry {i+1} from {source}: {getattr(entry, 'title', 'No title')}\")\n",
    "                        \n",
    "                        # Check if entry has required fields\n",
    "                        title = getattr(entry, 'title', 'No title')\n",
    "                        summary = getattr(entry, 'summary', getattr(entry, 'description', ''))\n",
    "                        \n",
    "                        if self.is_upgrade_related(title + \" \" + summary):\n",
    "                            article = {\n",
    "                                'source': source,\n",
    "                                'title': title,\n",
    "                                'link': getattr(entry, 'link', ''),\n",
    "                                'date': getattr(entry, 'published', getattr(entry, 'updated', 'No date')),\n",
    "                                'article_summary': summary\n",
    "                            }\n",
    "                            articles.append(article)\n",
    "                            logger.info(f\"Added upgrade-related article from {source}: {title}\")\n",
    "                \n",
    "                else:\n",
    "                    logger.warning(f\"No entries found in feed for {source}\")\n",
    "                    # Print more detailed feed information for debugging\n",
    "                    if hasattr(feed, 'bozo') and feed.bozo:\n",
    "                        logger.error(f\"Feed parsing error for {source}: {feed.bozo_exception}\")\n",
    "                    \n",
    "                    # Show raw content preview if feed is empty\n",
    "                    if response:\n",
    "                        logger.info(f\"Raw content preview for {source}:\")\n",
    "                        logger.info(response.text[:500] + \"...\" if len(response.text) > 500 else response.text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {source}: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "        \n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Total articles found: {len(articles)}\")\n",
    "        return articles\n",
    "    \n",
    "    def is_upgrade_related(self, text):\n",
    "        keywords = ['upgrade', 'update', 'fork', 'hardfork', 'testnet', \n",
    "                   'mainnet', 'release', 'version', 'protocol', 'network']\n",
    "        return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def get_flow_blog_rss(self):\n",
    "        \"\"\"Special handler for Flow blog since it doesn't have RSS\"\"\"\n",
    "        logger.info(\"Attempting to find RSS feed for Flow blog...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get('https://flow.com/blog', headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for RSS link in HTML head\n",
    "            rss_link = soup.find('link', {'type': 'application/rss+xml'})\n",
    "            if rss_link:\n",
    "                rss_url = rss_link.get('href')\n",
    "                logger.info(f\"Found RSS feed for Flow: {rss_url}\")\n",
    "                return rss_url\n",
    "            else:\n",
    "                logger.warning(\"No RSS feed found for Flow blog\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding Flow RSS feed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage with debugging\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = BlockchainNewsScraper()\n",
    "    \n",
    "    # Optional: Try to find Flow's actual RSS feed\n",
    "    flow_rss = scraper.get_flow_blog_rss()\n",
    "    if flow_rss:\n",
    "        scraper.sources['flow_blog'] = flow_rss\n",
    "    \n",
    "    articles = scraper.scrape_rss_feeds()\n",
    "    \n",
    "    # Print results summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if articles:\n",
    "        for article in articles:\n",
    "            print(f\"\\nSource: {article['source']}\")\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"Date: {article['date']}\")\n",
    "            print(f\"Link: {article['link']}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"No upgrade-related articles found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5c133-1b7b-4c2d-9784-a061b2b0c7c3",
   "metadata": {},
   "source": [
    "# Feed into AI\n",
    "\n",
    "Now that we have articles, we can feed this into AI to generate ideas. First we will import our claude API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e46ce1-19be-419c-b4cf-bf9fb8721f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A few months ago, we announced a renewed focus of Protocol on three strategic initiatives: Scale L1, Scale blobs, Improve UX. Following previous updates on Scale L1 and Scale blobs, this note relates to our “Improve UX” track, and its mission:  **Seamless, secure and permissionless experience across the Ethereum...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:57:19,733 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following up from Protocol Update 001, we’d like to introduce our approach to blob scaling. The L1 serves as a robust foundation for L2 systems to scale Ethereum, and a necessary component of secure L2 solutions is data availability provided by the L1. Data availability ensures that updates L2s make...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:57:47,722 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3>Most profitable SushiSwap liquidity pool ArbiFLUX-ETH — 162.44% APY</h3><blockquote>Transaction-incentivized Liquidity Pools (Monetary Velocity). Decentralizing inflation. On Arbitrum.</blockquote><p>Just a month after launch, <a href=\"https://analytics-arbitrum.sushi.com/tokens/0x64081252c497fcfec247a664e9d10ca8ed71b276\"><strong>ArbiFLUX-ETH</strong></a> has become the most profitable <em>APY</em> pool on <strong>SushiSwap</strong> (Arbitrum Layer). 🎉</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*d8zq50iv-YSY7lIC90uxrw.png\" /></figure><p>So far there have been 1,222 ArbiFLUX ‘high monetary velocity’ transfers. This throughput involves the ArbiFLUX/ETH and ArbiFLUX/FLUX liquidity pools on SushiSwap, which means amazing rewards (162% APY) for Liquidity Providers.</p><p>Also, 60% of the total ArbiFLUX supply has been burned by the community to counteract inflation using ArbiFLUX purchased from these pools. The market cap of ArbiFLUX just reached $20,000 USD, which is backed by $340,000 USD of L2 FLUX powering user-controlled ArbiFLUX Mints.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Hpjzg9rmcriv7rp-jUnZcg.png\" /></figure><p><strong>ArbiFLUX</strong> is a <strong>v2 currency</strong> that retains value while fulfilling the following criteria: transportability, utility, divisibility, scarcity, and durability.</p><p>Visit <a href=\"https://datamine-crypto.github.io/datamine-pro-portal/#/dashboard\">datamine.network</a> to start your own ArbiFLUX Mint.</p><p><strong>ArbiFLUX price tracker</strong>:</p><p><a href=\"https://www.defined.fi/arb/0xbf719d56c5f19ae0833adc4080befc48a9b415b5\">ArbiFLUX/WETH | $24.9273 | Defined</a></p><p><strong>Join Datamine in Discord</strong>:</p><p><a href=\"https://discord.me/dataminenetwork\">Datamine Network Discord Server</a></p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=8b717e5e7b2d\" width=\"1\" />\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:58:16,617 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p><strong>Binance</strong> has completed the integration of Arbitrum One Layer 2 Mainnet, a scaling solution for the Ethereum network (that has lower costs and faster transactions than on the Ethereum Mainnet).</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*UHtKCPCG6ja2IE5i.jpg\" /></figure><p>This move will allow projects like <a href=\"https://datamine-crypto.github.io/datamine-pro-portal/#/dashboard\"><strong>datamine.network</strong></a> to take full advantage of lower gas fees and instant transactions.</p><p><strong>$DAM</strong> price has already doubled from $0.10 to $0.21</p><ul><li>$FLUX (L1) has increased from $0.62 to $1.08</li><li>$FLUX (L2) has increased from $0.70 to $0.90</li><li>Newly launched ArbiFLUX is trading from $50 — $160 (pretty volatile due to low liquidity)</li></ul><p>All time high DAM Powering Mints (80.38%): $ 2,911,954 USD is locked on L1 to generate FLUX.</p><ul><li>Total DAM supply is 16,876,778; only 3,310,515 tokens in circulation.</li></ul><p>369,588.28 FLUX / $ 334,169.55 USDC (92.54% of L2 supply) is locked on L2, to generate ArbiFLUX.</p><p>Current APY:</p><ul><li>L1 layer (Ethereum): 30%</li><li>L2 layer (Arbitrum): 700% (should normalize as more ArbiFLUX is minted).</li></ul><p>Until next time.</p><img alt=\"\" height=\"1\" src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=a2998644367c\" width=\"1\" />\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 15:58:40,601 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import anthropic\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "model = os.getenv('ANTHROPIC_MODEL')\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "test_article = articles[0][\"article_summary\"]\n",
    "\n",
    "\n",
    "for a in articles:\n",
    "    summary = a.get('article_summary', None)[0].text\n",
    "\n",
    "    print(summary)\n",
    "    \n",
    "    if summary:\n",
    "        message = client.messages.create(\n",
    "            model=model,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"As a research agency experienced in user research can you take the content of this article and generate a proposal idea to perform user research?: {summary}\"\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "        a['summary'] = message.content\n",
    "\n",
    "    else:\n",
    "        print(\"No content in article!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e8435ba-3d1c-48ab-a44c-df726b4cf6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ PDF report generated successfully!\n",
      "📄 Report saved as: web3_news_report_20250831_160612.pdf\n",
      "📊 Total articles in report: 4\n",
      "\n",
      "📋 Report Summary:\n",
      "  • Ethereum Blog: 2 articles\n",
      "  • Arbitrum Medium: 2 articles\n",
      "PDF saved to: web3_news_report_20250831_160612.pdf\n"
     ]
    }
   ],
   "source": [
    "# Import and generate PDF report\n",
    "from pdf_generator import generate_news_pdf\n",
    "\n",
    "if 'articles' in locals() and articles:\n",
    "    pdf_path = generate_news_pdf(articles)\n",
    "    print(f\"PDF saved to: {pdf_path}\")\n",
    "else:\n",
    "    print(\"No articles found. Please run the scraper first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b46ee930-4237-4b97-af32-1e56ebe60c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# User Research Proposal: Improving Ethereum User Experience\\n\\n## Executive Summary\\n\\nThis proposal outlines a comprehensive user research initiative to support Ethereum\\'s \"Improve UX\" strategic track. The research will identify key pain points, barriers, and opportunities to create a more seamless, secure, and permissionless experience across the Ethereum ecosystem.\\n\\n## Research Objectives\\n\\n**Primary Objective:** Understand how to make Ethereum more accessible and user-friendly while maintaining its core principles of decentralization and security.\\n\\n**Secondary Objectives:**\\n- Map the current user journey across different Ethereum touchpoints\\n- Identify friction points that prevent mainstream adoption\\n- Understand security concerns and how they impact user behavior\\n- Explore mental models around permissionless systems\\n- Benchmark UX expectations against traditional web/financial services\\n\\n## Research Questions\\n\\n### Core Questions:\\n1. What are the primary barriers preventing users from adopting Ethereum-based applications?\\n2. How do users currently conceptualize and interact with decentralized systems?\\n3. What security vs. usability trade-offs are users willing to make?\\n4. Where do users experience the most friction in their Ethereum journey?\\n\\n### Supporting Questions:\\n- How do different user segments (crypto-native vs. newcomers) experience Ethereum differently?\\n- What language, terminology, and mental models resonate with users?\\n- How do users currently manage keys, wallets, and transactions?\\n- What are user expectations for transaction speed, cost, and reliability?\\n\\n## Proposed Methodology\\n\\n### Phase 1: Foundation Research (4-6 weeks)\\n- **Stakeholder interviews** with Protocol team members and ecosystem partners\\n- **Secondary research** analysis of existing UX studies in crypto/blockchain\\n- **Competitive analysis** of user experiences in traditional and crypto applications\\n\\n### Phase 2: User Discovery (6-8 weeks)\\n- **In-depth interviews** (n=30-40) with diverse user segments:\\n  - Crypto newcomers/never-users\\n  - Occasional users\\n  - Power users/developers\\n  - Users who abandoned Ethereum\\n- **Journey mapping sessions** to understand end-to-end experiences\\n- **Diary studies** to capture real-world usage patterns over time\\n\\n### Phase 3: Experience Evaluation (4-6 weeks)\\n- **Usability testing** of key user flows across popular Ethereum applications\\n- **Card sorting exercises** to understand user mental models\\n- **Prototype testing** of potential UX improvements\\n\\n### Phase 4: Validation & Strategy (3-4 weeks)\\n- **Survey research** (n=500+) to validate findings at scale\\n- **Workshop sessions** with Protocol team to prioritize opportunities\\n- **Roadmap development** for UX improvements\\n\\n## Target Participants\\n\\n### Primary Segments:\\n- **Crypto Curious** (40%): Users interested in crypto but haven\\'t used Ethereum\\n- **Ethereum Beginners** (30%): Users with <1 year of Ethereum experience\\n- **Intermediate Users** (20%): Regular Ethereum users (1-3 years experience)\\n- **Advanced Users** (10%): Power users, developers, and long-time participants\\n\\n### Demographic Considerations:\\n- Geographic diversity (North America, Europe, Asia, emerging markets)\\n- Age ranges (18-65+)\\n- Technical background variety\\n- Income/economic status diversity\\n\\n## Key Focus Areas\\n\\n1. **Wallet and Key Management**\\n   - Setup and onboarding experiences\\n   - Security practices and concerns\\n   - Recovery and backup processes\\n\\n2. **Transaction Experience**\\n   - Gas fee comprehension and prediction\\n   - Transaction status and confirmation\\n   - Error handling and recovery\\n\\n3. **DApp Discovery and Usage**\\n   - How users find and evaluate applications\\n   - Cross-application experiences\\n   - Trust and security assessment\\n\\n4. **Education and Support**\\n   - Learning resources and preferences\\n   - Community support utilization\\n   - Help-seeking behavior\\n\\n## Deliverables\\n\\n1. **Comprehensive Research Report** including:\\n   - Executive summary with key insights\\n   - Detailed findings by research phase\\n   - User personas and journey maps\\n   - Prioritized opportunity areas\\n\\n2. **UX Strategy Roadmap** featuring:\\n   - Short, medium, and long-term recommendations\\n   - Impact vs. effort prioritization matrix\\n   - Success metrics and KPIs\\n\\n3. **Design Principles & Guidelines** for:\\n   - Ethereum UX best practices\\n   - Language and terminology standards'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3e1f08-04fc-428d-83d7-bc4bdf731acb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "News Scraper",
   "language": "python",
   "name": "news_scraper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
