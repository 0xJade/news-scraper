{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd48437-248a-4e41-b084-db9cf19af9f7",
   "metadata": {},
   "source": [
    "# Ecosystem Report\n",
    "\n",
    "This report aims to scrape blogs and summarize news, generate proposals for how we can present research ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7cd729-4d20-4019-8b31-abe6052471a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 22:34:07,270 - INFO - Attempting to find RSS feed for Flow blog...\n",
      "2025-08-24 22:34:07,638 - WARNING - No RSS feed found for Flow blog\n",
      "2025-08-24 22:34:07,641 - INFO - \n",
      "==================================================\n",
      "2025-08-24 22:34:07,641 - INFO - Processing source: ethereum_blog\n",
      "2025-08-24 22:34:07,642 - INFO - URL: https://blog.ethereum.org/feed.xml\n",
      "2025-08-24 22:34:07,642 - INFO - Testing URL accessibility for ethereum_blog: https://blog.ethereum.org/feed.xml\n",
      "2025-08-24 22:34:07,963 - INFO - ethereum_blog - Status Code: 200\n",
      "2025-08-24 22:34:07,965 - INFO - ethereum_blog - Content-Type: application/xml\n",
      "2025-08-24 22:34:07,966 - INFO - ethereum_blog - Content Length: 478507 bytes\n",
      "2025-08-24 22:34:07,966 - INFO - Parsing feed for ethereum_blog...\n",
      "2025-08-24 22:34:08,430 - INFO - Analyzing feed structure for ethereum_blog\n",
      "2025-08-24 22:34:08,430 - INFO - Processing 581 entries from ethereum_blog\n",
      "2025-08-24 22:34:08,431 - INFO - Added upgrade-related article from ethereum_blog: Protocol Update 002 - Scale Blobs\n",
      "2025-08-24 22:34:08,431 - INFO - Added upgrade-related article from ethereum_blog: Join Us: EF Protocol Reddit AMA - August 29th, 2025\n",
      "2025-08-24 22:34:08,431 - INFO - Added upgrade-related article from ethereum_blog: Protocol Update 001 – Scale L1\n",
      "2025-08-24 22:34:08,431 - INFO - \n",
      "==================================================\n",
      "2025-08-24 22:34:08,432 - INFO - Processing source: arbitrum_medium\n",
      "2025-08-24 22:34:08,432 - INFO - URL: https://medium.com/feed/@arbitrum\n",
      "2025-08-24 22:34:08,432 - INFO - Testing URL accessibility for arbitrum_medium: https://medium.com/feed/@arbitrum\n",
      "2025-08-24 22:34:08,676 - INFO - arbitrum_medium - Status Code: 200\n",
      "2025-08-24 22:34:08,677 - INFO - arbitrum_medium - Content-Type: text/xml; charset=UTF-8\n",
      "2025-08-24 22:34:08,677 - INFO - arbitrum_medium - Content Length: 17843 bytes\n",
      "2025-08-24 22:34:08,681 - INFO - Parsing feed for arbitrum_medium...\n",
      "2025-08-24 22:34:08,912 - INFO - Analyzing feed structure for arbitrum_medium\n",
      "2025-08-24 22:34:08,914 - INFO - Processing 7 entries from arbitrum_medium\n",
      "2025-08-24 22:34:08,916 - INFO - Added upgrade-related article from arbitrum_medium: Most profitable SushiSwap liquidity pool ArbiFLUX-ETH — 162.44% APY\n",
      "2025-08-24 22:34:08,918 - INFO - Added upgrade-related article from arbitrum_medium: Binance, Arbitrum One Integration, Datamine Network\n",
      "2025-08-24 22:34:08,919 - INFO - Added upgrade-related article from arbitrum_medium: ArbiFLUX DeFi Burn Competition Starting November 18.\n",
      "2025-08-24 22:34:08,919 - INFO - Added upgrade-related article from arbitrum_medium: DeFi growth is more impressive than it seems.\n",
      "2025-08-24 22:34:08,924 - INFO - \n",
      "==================================================\n",
      "2025-08-24 22:34:08,927 - INFO - Processing source: flow_blog\n",
      "2025-08-24 22:34:08,928 - INFO - URL: https://www.onflow.org/post/rss.xml\n",
      "2025-08-24 22:34:08,930 - INFO - Testing URL accessibility for flow_blog: https://www.onflow.org/post/rss.xml\n",
      "2025-08-24 22:34:09,157 - ERROR - flow_blog - Request failed: HTTPSConnectionPool(host='www.onflow.org', port=443): Max retries exceeded with url: /post/rss.xml (Caused by SSLError(SSLError(1, '[SSL: TLSV1_ALERT_INTERNAL_ERROR] tlsv1 alert internal error (_ssl.c:1010)')))\n",
      "2025-08-24 22:34:09,160 - ERROR - Skipping flow_blog - URL not accessible\n",
      "2025-08-24 22:34:09,160 - INFO - \n",
      "==================================================\n",
      "2025-08-24 22:34:09,165 - INFO - Total articles found: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "\n",
      "Source: ethereum_blog\n",
      "Title: Protocol Update 002 - Scale Blobs\n",
      "Date: Fri, 22 Aug 2025 00:00:00 GMT\n",
      "Link: https://blog.ethereum.org/en/2025/08/22/protocol-update-002\n",
      "----------------------------------------\n",
      "\n",
      "Source: ethereum_blog\n",
      "Title: Join Us: EF Protocol Reddit AMA - August 29th, 2025\n",
      "Date: Fri, 15 Aug 2025 00:00:00 GMT\n",
      "Link: https://blog.ethereum.org/en/2025/08/15/protocol-ama\n",
      "----------------------------------------\n",
      "\n",
      "Source: ethereum_blog\n",
      "Title: Protocol Update 001 – Scale L1\n",
      "Date: Tue, 05 Aug 2025 00:00:00 GMT\n",
      "Link: https://blog.ethereum.org/en/2025/08/05/protocol-update-001\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: Most profitable SushiSwap liquidity pool ArbiFLUX-ETH — 162.44% APY\n",
      "Date: Wed, 15 Dec 2021 01:20:00 GMT\n",
      "Link: https://arbitrum.medium.com/most-profitable-sushiswap-liquidity-pool-arbiflux-eth-162-44-apy-8b717e5e7b2d?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: Binance, Arbitrum One Integration, Datamine Network\n",
      "Date: Sat, 20 Nov 2021 00:56:29 GMT\n",
      "Link: https://arbitrum.medium.com/binance-arbitrum-one-integration-datamine-network-a2998644367c?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: ArbiFLUX DeFi Burn Competition Starting November 18.\n",
      "Date: Sat, 13 Nov 2021 04:22:35 GMT\n",
      "Link: https://arbitrum.medium.com/arbiflux-defi-burn-competition-starting-november-18-231a8564ccbe?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n",
      "\n",
      "Source: arbitrum_medium\n",
      "Title: DeFi growth is more impressive than it seems.\n",
      "Date: Fri, 12 Nov 2021 02:21:41 GMT\n",
      "Link: https://arbitrum.medium.com/defi-growth-is-more-impressive-than-it-seems-aae57d84de50?source=rss-8cf0900f966a------2\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import feedparser\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# Set up logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BlockchainNewsScraper:\n",
    "    def __init__(self):\n",
    "        self.sources = {\n",
    "            'ethereum_blog': 'https://blog.ethereum.org/feed.xml',\n",
    "            'arbitrum_medium': 'https://medium.com/feed/@arbitrum',\n",
    "            # 'polygon_blog': 'https://blog.polygon.technology/feed',\n",
    "            # 'solana_news': 'https://solana.com/news/rss.xml',\n",
    "            # 'flow_blog': 'https://www.onflow.org/post/rss.xml'\n",
    "        }\n",
    "        \n",
    "        # Add headers to mimic a real browser\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'application/rss+xml, application/xml, text/xml, application/atom+xml',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "        }\n",
    "    \n",
    "    def test_url_accessibility(self, url, source_name):\n",
    "        \"\"\"Test if URL is accessible and what type of content it returns\"\"\"\n",
    "        logger.info(f\"Testing URL accessibility for {source_name}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            \n",
    "            logger.info(f\"{source_name} - Status Code: {response.status_code}\")\n",
    "            logger.info(f\"{source_name} - Content-Type: {response.headers.get('content-type', 'Unknown')}\")\n",
    "            logger.info(f\"{source_name} - Content Length: {len(response.content)} bytes\")\n",
    "            \n",
    "            # Check if it's actually RSS/XML content\n",
    "            content_type = response.headers.get('content-type', '').lower()\n",
    "            is_xml = 'xml' in content_type or 'rss' in content_type or 'atom' in content_type\n",
    "            \n",
    "            if not is_xml:\n",
    "                # Check if content starts with XML declaration or RSS tags\n",
    "                content_preview = response.text[:200]\n",
    "                logger.info(f\"{source_name} - Content preview: {content_preview}\")\n",
    "                \n",
    "                if not any(tag in content_preview.lower() for tag in ['<rss', '<feed', '<?xml']):\n",
    "                    logger.warning(f\"{source_name} - This appears to be HTML, not RSS/XML!\")\n",
    "            \n",
    "            return response.status_code == 200, response\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"{source_name} - Request failed: {e}\")\n",
    "            return False, None\n",
    "    \n",
    "    def analyze_feed_structure(self, feed, source_name):\n",
    "        \"\"\"Analyze the feed structure to understand what's available\"\"\"\n",
    "        logger.info(f\"Analyzing feed structure for {source_name}\")\n",
    "        \n",
    "        # # Check feed metadata\n",
    "        # if hasattr(feed, 'feed'):\n",
    "        #     logger.info(f\"{source_name} - Feed title: {getattr(feed.feed, 'title', 'No title')}\")\n",
    "        #     logger.info(f\"{source_name} - Feed description: {getattr(feed.feed, 'description', 'No description')}\")\n",
    "        #     logger.info(f\"{source_name} - Feed version: {getattr(feed, 'version', 'Unknown version')}\")\n",
    "        \n",
    "        # # Check entries\n",
    "        # logger.info(f\"{source_name} - Number of entries: {len(feed.entries)}\")\n",
    "        \n",
    "        if feed.entries:\n",
    "            # Analyze first entry structure\n",
    "            first_entry = feed.entries[0]\n",
    "            # logger.info(f\"{source_name} - First entry keys: {list(first_entry.keys())}\")\n",
    "            # logger.info(f\"{source_name} - First entry title: {getattr(first_entry, 'title', 'No title')}\")\n",
    "        \n",
    "        # Check for parsing errors\n",
    "        if hasattr(feed, 'bozo') and feed.bozo:\n",
    "            logger.warning(f\"{source_name} - Feed has parsing issues: {feed.bozo_exception}\")\n",
    "    \n",
    "    def scrape_rss_feeds(self):\n",
    "        articles = []\n",
    "        \n",
    "        for source, url in self.sources.items():\n",
    "            logger.info(f\"\\n{'='*50}\")\n",
    "            logger.info(f\"Processing source: {source}\")\n",
    "            logger.info(f\"URL: {url}\")\n",
    "            \n",
    "            try:\n",
    "                # First, test URL accessibility\n",
    "                is_accessible, response = self.test_url_accessibility(url, source)\n",
    "                \n",
    "                if not is_accessible:\n",
    "                    logger.error(f\"Skipping {source} - URL not accessible\")\n",
    "                    continue\n",
    "                \n",
    "                # Parse the feed\n",
    "                logger.info(f\"Parsing feed for {source}...\")\n",
    "                feed = feedparser.parse(url)\n",
    "                \n",
    "                # Analyze feed structure\n",
    "                self.analyze_feed_structure(feed, source)\n",
    "                \n",
    "                # Check if feed has entries\n",
    "                if len(feed.entries) > 0:\n",
    "                    logger.info(f\"Processing {len(feed.entries)} entries from {source}\")\n",
    "                    \n",
    "                    for i, entry in enumerate(feed.entries[:5]):  # Latest 5 articles\n",
    "                        logger.debug(f\"Processing entry {i+1} from {source}: {getattr(entry, 'title', 'No title')}\")\n",
    "                        \n",
    "                        # Check if entry has required fields\n",
    "                        title = getattr(entry, 'title', 'No title')\n",
    "                        summary = getattr(entry, 'summary', getattr(entry, 'description', ''))\n",
    "                        \n",
    "                        if self.is_upgrade_related(title + \" \" + summary):\n",
    "                            article = {\n",
    "                                'source': source,\n",
    "                                'title': title,\n",
    "                                'link': getattr(entry, 'link', ''),\n",
    "                                'date': getattr(entry, 'published', getattr(entry, 'updated', 'No date')),\n",
    "                                'summary': summary\n",
    "                            }\n",
    "                            articles.append(article)\n",
    "                            logger.info(f\"Added upgrade-related article from {source}: {title}\")\n",
    "                \n",
    "                else:\n",
    "                    logger.warning(f\"No entries found in feed for {source}\")\n",
    "                    # Print more detailed feed information for debugging\n",
    "                    if hasattr(feed, 'bozo') and feed.bozo:\n",
    "                        logger.error(f\"Feed parsing error for {source}: {feed.bozo_exception}\")\n",
    "                    \n",
    "                    # Show raw content preview if feed is empty\n",
    "                    if response:\n",
    "                        logger.info(f\"Raw content preview for {source}:\")\n",
    "                        logger.info(response.text[:500] + \"...\" if len(response.text) > 500 else response.text)\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error scraping {source}: {e}\")\n",
    "                import traceback\n",
    "                logger.error(traceback.format_exc())\n",
    "        \n",
    "        logger.info(f\"\\n{'='*50}\")\n",
    "        logger.info(f\"Total articles found: {len(articles)}\")\n",
    "        return articles\n",
    "    \n",
    "    def is_upgrade_related(self, text):\n",
    "        keywords = ['upgrade', 'update', 'fork', 'hardfork', 'testnet', \n",
    "                   'mainnet', 'release', 'version', 'protocol', 'network']\n",
    "        return any(keyword.lower() in text.lower() for keyword in keywords)\n",
    "    \n",
    "    def get_flow_blog_rss(self):\n",
    "        \"\"\"Special handler for Flow blog since it doesn't have RSS\"\"\"\n",
    "        logger.info(\"Attempting to find RSS feed for Flow blog...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get('https://flow.com/blog', headers=self.headers)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for RSS link in HTML head\n",
    "            rss_link = soup.find('link', {'type': 'application/rss+xml'})\n",
    "            if rss_link:\n",
    "                rss_url = rss_link.get('href')\n",
    "                logger.info(f\"Found RSS feed for Flow: {rss_url}\")\n",
    "                return rss_url\n",
    "            else:\n",
    "                logger.warning(\"No RSS feed found for Flow blog\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error finding Flow RSS feed: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage with debugging\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = BlockchainNewsScraper()\n",
    "    \n",
    "    # Optional: Try to find Flow's actual RSS feed\n",
    "    flow_rss = scraper.get_flow_blog_rss()\n",
    "    if flow_rss:\n",
    "        scraper.sources['flow_blog'] = flow_rss\n",
    "    \n",
    "    articles = scraper.scrape_rss_feeds()\n",
    "    \n",
    "    # Print results summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"FINAL RESULTS SUMMARY\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if articles:\n",
    "        for article in articles:\n",
    "            print(f\"\\nSource: {article['source']}\")\n",
    "            print(f\"Title: {article['title']}\")\n",
    "            print(f\"Date: {article['date']}\")\n",
    "            print(f\"Link: {article['link']}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"No upgrade-related articles found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5c133-1b7b-4c2d-9784-a061b2b0c7c3",
   "metadata": {},
   "source": [
    "# Feed into AI\n",
    "\n",
    "Now that we have articles, we can feed this into AI to generate ideas. First we will import our claude API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ed1e16f-f172-4d64-a1c2-49e406781d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "model = os.getenv('ANTHROPIC_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e1bd0-d3f5-4f2a-868b-1ed6cb53ef39",
   "metadata": {},
   "source": [
    "# Query Claude for Proposal Ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90e46ce1-19be-419c-b4cf-bf9fb8721f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 22:41:18,342 - INFO - HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlock(citations=None, text=\"# User Research Proposal: Understanding Developer and User Perspectives on Ethereum's Blob Scaling Implementation\\n\\n## Executive Summary\\nThis research proposal aims to investigate user perceptions, adoption barriers, and practical implementation challenges related to Ethereum's blob scaling approach for Layer 2 (L2) solutions. The study will focus on understanding how developers, L2 operators, and end-users experience data availability improvements and scaling benefits.\\n\\n## Research Objectives\\n\\n### Primary Objectives\\n- Understand developer adoption patterns and implementation challenges with blob scaling\\n- Assess user perception of performance improvements in L2 transactions\\n- Identify barriers to blob scaling adoption across different user segments\\n\\n### Secondary Objectives\\n- Map the user journey for implementing blob-based data availability solutions\\n- Evaluate the effectiveness of current documentation and developer resources\\n- Understand cost-benefit perceptions among L2 operators and developers\\n\\n## Target User Segments\\n\\n1. **L2 Protocol Developers** - Teams building and maintaining L2 scaling solutions\\n2. **DApp Developers** - Application developers integrating with L2 networks\\n3. **L2 Network Operators** - Organizations running L2 infrastructure\\n4. **End Users** - Individuals using L2 applications and experiencing scaling benefits\\n5. **Enterprise Users** - Businesses evaluating L2 solutions for their applications\\n\\n## Research Methodology\\n\\n### Phase 1: Discovery Research (Weeks 1-3)\\n- **In-depth interviews** (12-15 participants) with L2 developers and operators\\n- **Survey deployment** (200+ responses) across developer communities\\n- **Desk research** on existing implementation patterns and community feedback\\n\\n### Phase 2: Usability & Experience Research (Weeks 4-6)\\n- **Task-based usability sessions** with developers implementing blob scaling\\n- **Journey mapping workshops** with L2 teams\\n- **Performance perception studies** with end users\\n\\n### Phase 3: Validation & Synthesis (Weeks 7-8)\\n- **Focus groups** with mixed user segments\\n- **Card sorting exercises** for prioritizing improvement areas\\n- **Data analysis and insight synthesis**\\n\\n## Key Research Questions\\n\\n### For Developers & Operators:\\n- What are the primary technical challenges in implementing blob scaling?\\n- How do current tools and documentation support the implementation process?\\n- What cost and performance benefits have been realized?\\n- What additional resources or support would accelerate adoption?\\n\\n### For End Users:\\n- Do users perceive improvements in transaction speed and costs?\\n- How does the user experience compare across different L2 implementations?\\n- What concerns or confusion exist about data availability and security?\\n\\n### For All Segments:\\n- What factors influence the decision to adopt blob scaling solutions?\\n- How well understood are the trade-offs between different scaling approaches?\\n\\n## Deliverables\\n\\n1. **Executive Summary Report** - Key findings and strategic recommendations\\n2. **User Persona Profiles** - Detailed profiles of each user segment\\n3. **Journey Maps** - Implementation and user experience flows\\n4. **Usability Findings Report** - Specific UX/UI improvement recommendations\\n5. **Adoption Framework** - Guidelines for improving blob scaling adoption\\n6. **Developer Resource Audit** - Assessment of current tools and documentation gaps\\n\\n## Timeline & Budget\\n\\n**Duration:** 8 weeks\\n**Estimated Budget:** $45,000 - $60,000\\n\\n- Research design and recruitment: $8,000\\n- Data collection and moderation: $25,000\\n- Analysis and reporting: $15,000\\n- Travel and incentives: $7,000\\n- Project management and overhead: $5,000-$20,000\\n\\n## Success Metrics\\n\\n- Identification of top 3-5 adoption barriers with actionable solutions\\n- 85%+ satisfaction rate from research participants\\n- Delivery of implementable recommendations for improving developer experience\\n- Clear prioritization framework for product development investments\\n\\n## Next Steps\\n\\n1. Stakeholder alignment on research scope and objectives\\n2. Participant recruitment across target segments\\n3. Research protocol finalization and tool preparation\\n4. Kick-off meeting and research execution\\n\\nThis research will provide actionable insights to optimize blob scaling adoption and improve the overall Ethereum L2 ecosystem experience.\", type='text')]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "client = anthropic.Anthropic()\n",
    "test_article = articles[0][\"summary\"]\n",
    "\n",
    "message = client.messages.create(\n",
    "    model=model,\n",
    "    max_tokens=1000,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"As a research agency experienced in user research can you take the content of this article and generate a proposal idea to perform user research?: {test_article}\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4592c994-e196-40f3-9994-78fd0d16478b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'ethereum_blog',\n",
       " 'title': 'Protocol Update 002 - Scale Blobs',\n",
       " 'link': 'https://blog.ethereum.org/en/2025/08/22/protocol-update-002',\n",
       " 'date': 'Fri, 22 Aug 2025 00:00:00 GMT',\n",
       " 'summary': 'Following up from Protocol Update 001, we’d like to introduce our approach to blob scaling. The L1 serves as a robust foundation for L2 systems to scale Ethereum, and a necessary component of secure L2 solutions is data availability provided by the L1. Data availability ensures that updates L2s make...'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eef42d6-fdbb-45cf-808a-dba0192efa59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "News Scraper",
   "language": "python",
   "name": "news_scraper_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
